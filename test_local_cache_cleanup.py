#!/usr/bin/env python3
"""
æµ‹è¯•æœ¬åœ°ç¼“å­˜çš„åå°è‡ªåŠ¨æ¸…ç†åŠŸèƒ½
"""
import time
import logging
import os

# è®¾ç½®æ—¥å¿—çº§åˆ«
os.environ["LITELLM_LOG"] = "DEBUG"

from litellm._logging import verbose_proxy_logger
from litellm.llms.vertex_ai.context_caching.local_cache_manager import (
    get_cache_manager,
)

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º DEBUG
verbose_proxy_logger.setLevel(logging.DEBUG)

# æ·»åŠ æ§åˆ¶å° handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
verbose_proxy_logger.addHandler(console_handler)

print("=" * 80)
print("æµ‹è¯•æœ¬åœ°ç¼“å­˜åå°è‡ªåŠ¨æ¸…ç†åŠŸèƒ½")
print("=" * 80)

# è·å–ç¼“å­˜ç®¡ç†å™¨ï¼ˆå¸¦æœ‰ 10 ç§’æ¸…ç†é—´éš”ç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
cache_manager = get_cache_manager()

# æ˜¾ç¤ºåˆå§‹ç»Ÿè®¡
print("\nğŸ“Š åˆå§‹ç¼“å­˜ç»Ÿè®¡:")
stats = cache_manager.get_stats()
for key, value in stats.items():
    print(f"  {key}: {value}")

# æ·»åŠ ä¸€äº›æµ‹è¯•ç¼“å­˜ï¼ˆTTL=15ç§’ï¼‰
print("\nâ• æ·»åŠ  5 ä¸ªæµ‹è¯•ç¼“å­˜ (TTL=15ç§’)...")
for i in range(5):
    cache_manager.set_cache(
        cache_key=f"test_cache_{i}",
        cache_id=f"projects/test/locations/global/cachedContents/cache_{i}",
        ttl_seconds=15,
        vertex_project="test-project",
        vertex_location="global",
        custom_llm_provider="vertex_ai"
    )
    print(f"  âœ“ æ·»åŠ ç¼“å­˜ test_cache_{i}")

# æ˜¾ç¤ºæ·»åŠ åç»Ÿè®¡
print("\nğŸ“Š æ·»åŠ åç¼“å­˜ç»Ÿè®¡:")
stats = cache_manager.get_stats()
for key, value in stats.items():
    print(f"  {key}: {value}")

# æµ‹è¯•æŸ¥è¯¢ç¼“å­˜
print("\nğŸ” æµ‹è¯•æŸ¥è¯¢ç¼“å­˜ test_cache_0:")
result = cache_manager.get_cache(
    cache_key="test_cache_0",
    vertex_project="test-project",
    vertex_location="global",
    custom_llm_provider="vertex_ai"
)
print(f"  æŸ¥è¯¢ç»“æœ: {result}")

# ç­‰å¾… 20 ç§’è®©ç¼“å­˜è¿‡æœŸ
print("\nâ³ ç­‰å¾… 20 ç§’è®©ç¼“å­˜è¿‡æœŸ...")
for i in range(20, 0, -1):
    print(f"  å€’è®¡æ—¶: {i} ç§’", end='\r')
    time.sleep(1)
print("\n")

# å†æ¬¡æŸ¥è¯¢ï¼ˆåº”è¯¥è§¦å‘æƒ°æ€§åˆ é™¤ï¼‰
print("ğŸ” å†æ¬¡æŸ¥è¯¢è¿‡æœŸçš„ç¼“å­˜ test_cache_0:")
result = cache_manager.get_cache(
    cache_key="test_cache_0",
    vertex_project="test-project",
    vertex_location="global",
    custom_llm_provider="vertex_ai"
)
print(f"  æŸ¥è¯¢ç»“æœ: {result} (åº”è¯¥æ˜¯ None)")

# æ˜¾ç¤ºç»Ÿè®¡ï¼ˆåº”è¯¥çœ‹åˆ°ä¸€ä¸ªè¢«æƒ°æ€§åˆ é™¤ï¼‰
print("\nğŸ“Š æƒ°æ€§åˆ é™¤åç»Ÿè®¡:")
stats = cache_manager.get_stats()
for key, value in stats.items():
    print(f"  {key}: {value}")

# æµ‹è¯•åå°æ¸…ç†åŠŸèƒ½
# æ³¨æ„ï¼šé»˜è®¤æ¸…ç†é—´éš”æ˜¯ 300 ç§’ï¼ˆ5åˆ†é’Ÿï¼‰
# è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨è§¦å‘æ¸…ç†æ¥æ¼”ç¤º
print("\nğŸ§¹ æ‰‹åŠ¨è§¦å‘è¿‡æœŸç¼“å­˜æ¸…ç†:")
removed = cache_manager.cleanup_expired()
print(f"  åˆ é™¤äº† {removed} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")

# æœ€ç»ˆç»Ÿè®¡
print("\nğŸ“Š æ¸…ç†åæœ€ç»ˆç»Ÿè®¡:")
stats = cache_manager.get_stats()
for key, value in stats.items():
    print(f"  {key}: {value}")

# æµ‹è¯• shutdown
print("\nğŸ›‘ æµ‹è¯•ä¼˜é›…å…³é—­:")
cache_manager.shutdown()

print("\n" + "=" * 80)
print("âœ… æµ‹è¯•å®Œæˆï¼")
print("=" * 80)
print("\nè¯´æ˜ï¼š")
print("1. åå°æ¸…ç†çº¿ç¨‹åœ¨åˆ›å»º cache_manager æ—¶è‡ªåŠ¨å¯åŠ¨")
print("2. é»˜è®¤æ¯ 5 åˆ†é’Ÿï¼ˆ300ç§’ï¼‰æ¸…ç†ä¸€æ¬¡è¿‡æœŸç¼“å­˜")
print("3. å½“æŸ¥è¯¢ç¼“å­˜æ—¶ï¼Œå¦‚æœè¿‡æœŸä¼šè¢«æƒ°æ€§åˆ é™¤ï¼ˆLazy Deletionï¼‰")
print("4. åå°çº¿ç¨‹æ˜¯å®ˆæŠ¤çº¿ç¨‹ï¼Œä¸ä¼šé˜»æ­¢ç¨‹åºé€€å‡º")
print("5. å¯ä»¥è°ƒç”¨ shutdown() ä¼˜é›…å…³é—­åå°çº¿ç¨‹")
